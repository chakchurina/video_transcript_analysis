{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa361f0e",
   "metadata": {},
   "source": [
    "# Experiment report \n",
    "This is a playground where I experimented and tested ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0d1ec",
   "metadata": {},
   "source": [
    "## Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc66708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose file for the playground  \n",
    "\n",
    "transcript_files = [\n",
    "    \"2024 Rolls-Royce Spectre Review.csv\",\n",
    "    \"Apple Vision Pro Impressions.csv\",\n",
    "    \"George Hotz.csv\",\n",
    "    \"The END of Sam Bankman Fried.csv\",\n",
    "    \"Why is LinkedIn so weird.csv\"\n",
    "]\n",
    "\n",
    "file = transcript_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"data/transcripts\"\n",
    "\n",
    "file_path = os.path.join(folder_path, file)\n",
    "raw_df = pd.read_csv(file_path)\n",
    "raw_df.rename(columns={'length': 'time'}, inplace=True)\n",
    "\n",
    "print(raw_df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    \"\"\"Removes punctuation, converts to lowercase, and splits into words.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "raw_df['tokens'] = raw_df['sentence'].apply(clean_tokenize)\n",
    "raw_df['length'] = raw_df['tokens'].apply(len)\n",
    "print(raw_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "API_KEY = \"YOUR_KEY\"\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=API_KEY,\n",
    ")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=MODEL\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "raw_df['embedding'] = raw_df['sentence'].apply(get_embeddings)\n",
    "print(raw_df['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69564b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_distance(embeddings):\n",
    "    cos_distances = [None]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        cos_distance = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]\n",
    "        cos_distances.append(cos_distance)\n",
    "    return cos_distances\n",
    "\n",
    "raw_df['cos_dist'] = cosine_distance(raw_df['embedding'].tolist())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(raw_df['cos_dist'], marker='o', linestyle='-')\n",
    "plt.xlabel('Sentence Index')\n",
    "plt.ylabel('Cosine Distance')\n",
    "plt.title('Cosine Distance Between Consecutive Sentences')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d620b",
   "metadata": {},
   "source": [
    "Here, I want to look at the data. We read texts sequentially, and I was curious to inspect the similarities of embeddings to understand if there's a way to further clean the data. Or maybe discover some insights from it.\n",
    "\n",
    "From here we see that identical sentences (like \"Cringe. Cringe. Cringe.\") will complicate processing â€” I'll try to merge them, but this will obfuscate finding timecodes in data, so I'll have to abandon the idea. The following section, where I restructure the dataset to eliminate similarities, did not make it into the application code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6f1ca",
   "metadata": {},
   "source": [
    "## Reforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfd4f1",
   "metadata": {},
   "source": [
    "Thought that sentences of 35+ tokens are rather long and could be split. As well as 5- could be merged. Here I go without calculating any stats on the text, just playing around with the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = 35\n",
    "short = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging sentences if they are close semantically anf short \n",
    "\n",
    "quantile = raw_df['cos_dist'].quantile(0.8)\n",
    "close_indices = raw_df.index[raw_df['cos_dist'] > quantile].tolist()\n",
    "# print(close_indices)\n",
    "\n",
    "sentences = [raw_df.loc[0, 'sentence']]\n",
    "times = [raw_df.loc[0, 'time']]\n",
    "\n",
    "i = 1\n",
    "while i < len(raw_df):    \n",
    "    current = raw_df.loc[i, 'sentence']\n",
    "    current_t = raw_df.loc[i, 'time']\n",
    "    length = raw_df.loc[i, 'length']\n",
    "\n",
    "    previous = sentences[-1]\n",
    "    previous_t = times[-1]\n",
    "    \n",
    "    # if it's short and similar, concatenate it \n",
    "    if i in close_indices and length <= short:\n",
    "        sentences[-1] = previous + \" \" + current\n",
    "        times[-1] = previous_t + current_t\n",
    "        \n",
    "    # if starts with ..., concatenate it\n",
    "    elif previous.endswith('...') and current.startswith('...'):\n",
    "        sentences[-1] = previous[:-3] + \" \" + current[3:]\n",
    "        times[-1] = previous_t + current_t\n",
    "    \n",
    "    # leave it as is if it's fine \n",
    "    else:        \n",
    "        sentences.append(current)\n",
    "        times.append(current_t)\n",
    "    i += 1\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df['sentence'] = sentences\n",
    "df['time'] = times\n",
    "\n",
    "print(df[-10:-1])\n",
    "print(\"\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculating embeddings and all the colums for the new dataframe \n",
    "\n",
    "df['tokens'] = df['sentence'].apply(clean_tokenize)\n",
    "df['tempo'] = df['tokens'].apply(len) / df['time']\n",
    "df['length'] = df['tokens'].apply(len)\n",
    "df['question'] = df['sentence'].str.contains('\\?')\n",
    "df['embedding'] = df['sentence'].apply(get_embeddings)\n",
    "\n",
    "df.info()\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['sentence'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31412a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['sentence'][77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out start and finish time for every sentence in a new df \n",
    "\n",
    "start_times = [0]\n",
    "end_times = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i > 0:\n",
    "        start_time = start_times[i-1] + df.loc[i-1, 'time']\n",
    "        start_times.append(start_time)\n",
    "    end_time = start_times[i] + df.loc[i, 'time']\n",
    "    end_times.append(end_time)\n",
    "\n",
    "df['start_time'] = start_times\n",
    "df['end_time'] = end_times\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[74])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1da5c3",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "Figuring out if sentiment analysis can reveal some knowledge about the text. \n",
    "\n",
    "Before choosing huggingface, I tried to TextBlob and Vader as something local and lightweight, but was not satisfied with results. I didn't calculate any metrics because I knew I had limited time. So I simply disagreed with these models outputs. In a production setting, I would try to get metrics on some labeled dataset similar to my data.\n",
    "\n",
    "For now I settled with Roberta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        scores = softmax(logits, dim=1)\n",
    "        scores_dict = {label: score.item() for label, score in zip(['negative', 'neutral', 'positive'], scores[0])}\n",
    "        return scores_dict\n",
    "    \n",
    "    def apply_to_dataframe(self, df, text_column):\n",
    "        non_neutrals, positives, negatives = [], [], []\n",
    "        \n",
    "        for text in df[text_column]:\n",
    "            sentiment_scores = self.predict_sentiment(text)\n",
    "            non_neutrals.append(1 - sentiment_scores['neutral'])\n",
    "            positives.append(sentiment_scores['positive'])\n",
    "            negatives.append(sentiment_scores['negative'])\n",
    "            \n",
    "        df['emotion_score'] = non_neutrals\n",
    "        df['positive_score'] = positives\n",
    "        df['negative_score'] = negatives\n",
    "\n",
    "\n",
    "analyzer = SentimentAnalyzer()\n",
    "analyzer.apply_to_dataframe(df, 'sentence')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e34620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aa3e5",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9af97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tempo = df['tempo'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e598509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_and_tempo(df):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.plot(df.index, df['emotion_score'], color='red', label='Roberta')\n",
    "    \n",
    "    plt.plot(df.index, df['tempo'], color='green', label='Tempo')\n",
    "    plt.axhline(y=mean_tempo, color='green', linestyle='-', label='Mean tempo')\n",
    "\n",
    "    plt.xlabel('Sentence Number')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Polarity, Subjectivity, and Tempo across Sentences')\n",
    "    plt.legend()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['question']:\n",
    "            plt.axvline(x=index, color='blue', label='Question Mark', linestyle='--')\n",
    "    \n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    plt.xticks(df.index[::2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_sentiment_and_tempo(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6cc52",
   "metadata": {},
   "source": [
    "Blue vertical lines are question marks. I wanted to see if parameters correspond to each other, not seriously, just visually. NB: I called 'emotion_score' all probably non-neutral (1 - neutral_score) sentences. This way I dont' care about the actual tone of the statement, rather see that it stands out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c933c8c",
   "metadata": {},
   "source": [
    "### Non-neutral statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_non_neutral_indices = sorted(df['emotion_score'].abs().nlargest(8).index.tolist())\n",
    "print(\"Top 8 RoBERTa Non-Neutral Sentences:\", top_non_neutral_indices)\n",
    "\n",
    "print(\"\\n\")\n",
    "for index in top_non_neutral_indices:\n",
    "    print(f\"{index}: {df.loc[index, 'sentence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6e075",
   "metadata": {},
   "source": [
    "### Non-neutral questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_threshold = df['emotion_score'].quantile(0.5)  # The upper part of the distribution\n",
    "\n",
    "questions = df[(df['question']) & (df['emotion_score'] > emotional_threshold)]\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    print(f\"{index}: {row['sentence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee856ec5",
   "metadata": {},
   "source": [
    "### Non-neutral tone & fast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastest = df['tempo'].quantile(0.75)\n",
    "\n",
    "fastest_emotional = df[(df['emotion_score'] > emotional_threshold) & (df['tempo'] > fastest)]\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    print(f\"{index}: {row['sentence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499ea3b",
   "metadata": {},
   "source": [
    "Interestingly, all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e411c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_5_slowest = df.sort_values(by='tempo', ascending=True).head(5).index.tolist()\n",
    "\n",
    "# Thinking that pauses might emphasize previous statements and being careless with indexation\n",
    "for i in top_5_slowest:\n",
    "    print(f\"{i - 2}: {df.loc[i - 2, 'sentence']}\")\n",
    "    print(f\"{i - 1}: {df.loc[i - 1, 'sentence']}\")\n",
    "    print(f\"{i}: {df.loc[i, 'sentence']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bedbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ee4be",
   "metadata": {},
   "source": [
    "## Semantic connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfee7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do questions have answers?\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_statements(df):\n",
    "    questions_df = df[df['question'] == True]\n",
    "    statements_df = df[df['question'] == False]\n",
    "    \n",
    "    question_embeddings = list(questions_df['embedding'])\n",
    "    statement_embeddings = list(statements_df['embedding'])\n",
    "    \n",
    "    closest_statements = {}\n",
    "    \n",
    "    for index, question_embedding in questions_df.iterrows():\n",
    "        similarities = cosine_similarity([question_embedding['embedding']], statement_embeddings)\n",
    "        top_5_indices = similarities[0].argsort()[-3:][::-1]\n",
    "        \n",
    "#         top_5_indices = np.append(np.sort(top_5_indices[1:]), top_5_indices[0])\n",
    "        \n",
    "        closest_sentences = statements_df.iloc[top_5_indices]['sentence'].values\n",
    "        closest_statements[question_embedding['sentence']] = closest_sentences\n",
    "    \n",
    "    return closest_statements\n",
    "\n",
    "closest_statements = find_closest_statements(df)\n",
    "for question, statements in closest_statements.items():\n",
    "    text = ' '.join(statements)\n",
    "    print(f\"Question: {question}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9012a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_threshold=0.5\n",
    "emotional_indexes = df[df['emotion_score'] > emotion_threshold].index\n",
    "print(emotional_indexes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for intros in the text as they might form nice openin of the video\n",
    "\n",
    "def find_intros(df):\n",
    "    similarity_threshold=0.765  # added ad-hoc threshold, very sorry\n",
    "    \n",
    "    request = \"My name is Ankit Singla and I'm a full-time blogger. I blog about blogging. I'm Karen, an entrepreneur and VC consultant. Paul ErdÅ‘s was a Hungarian mathematician. He was one of the most prolific mathematicians and producers of mathematical conjectures of the 20th century. This is Maria and she is an ML Engineer at Rask\"\n",
    "    request_embedding = get_embeddings(request)\n",
    "    request_embedding = np.array(request_embedding).reshape(1, -1)\n",
    "\n",
    "    sentence_similarities = []\n",
    "    for index, row in df.iterrows():\n",
    "        embedding = np.array(row['embedding']).reshape(1, -1)\n",
    "        similarity = cosine_similarity(embedding, request_embedding)[0][0]\n",
    "#         sentence_similarities.append((index, row['sentence'], similarity))\n",
    "        if similarity > similarity_threshold:\n",
    "            sentence_similarities.append((index, row['sentence'], similarity))\n",
    "\n",
    "    sorted_sentences = sorted(sentence_similarities, key=lambda x: x[2], reverse=True)\n",
    "    print(sorted_sentences)\n",
    "    return [{i: sentence} for i, sentence, _ in sorted_sentences]\n",
    "\n",
    "intros = find_intros(df)\n",
    "print(intros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7f900",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "See if clustering works for this task. Looks inconclusive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caacc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "embeddings_array = np.array(list(df['embedding']))\n",
    "\n",
    "# Getting the optimal number of clusters using silhouette score\n",
    "silhouette_scores = []\n",
    "for n_clusters in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings_array)\n",
    "    score = silhouette_score(embeddings_array, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_clusters = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "for cluster in sorted(df['cluster'].unique()):\n",
    "    sentences = df[df['cluster'] == cluster].sort_index()['sentence']\n",
    "    for sentence in sentences:\n",
    "        print(f\"{sentence}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39053e99",
   "metadata": {},
   "source": [
    "# Breaking into paragraphs \n",
    "Instead of thinking about breaking the text into unrelated and out-of-context thematic clusters, it's better to assume that in practice texts already come with a structure. Therefore, it should be divided into paragraphs, even if the division turns out to be imprecise.\n",
    "\n",
    "The matrix lookes different on raw data, remember these are not original sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042e480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "embeddings_matrix = np.array(df['embedding'].tolist())\n",
    "cosine_sim_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "sns.heatmap(cosine_sim_matrix).set_title('Cosine similarities matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661450c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking for split points\n",
    "\n",
    "import math\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "def rev_sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + math.exp(0.5 * x))\n",
    "\n",
    "def activate_similarities(similarities: np.array, p_size=10) -> np.array:\n",
    "    x = np.linspace(-10, 10, p_size)\n",
    "    y = np.vectorize(rev_sigmoid)\n",
    "    activation_weights = np.pad(y(x), (0, similarities.shape[0] - p_size), 'constant')\n",
    "    diagonals = [similarities.diagonal(each) for each in range(1, similarities.shape[0])]\n",
    "    diagonals = [np.pad(each, (0, similarities.shape[0] - len(each)), 'constant') for each in diagonals]\n",
    "    diagonals = np.stack(diagonals)\n",
    "    diagonals = diagonals * activation_weights[:diagonals.shape[0]].reshape(-1, 1)\n",
    "    activated_similarities = np.sum(diagonals, axis=0)\n",
    "    return activated_similarities\n",
    "\n",
    "activated_similarities = activate_similarities(cosine_sim_matrix, p_size=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "minimas = argrelextrema(activated_similarities, np.less, order=2)\n",
    "sns.lineplot(y=activated_similarities, x=range(len(activated_similarities)), ax=ax).set_title('Relative minima')\n",
    "plt.vlines(x=minimas, ymin=min(activated_similarities), ymax=max(activated_similarities), colors='purple', ls='--', lw=2, label='Split Points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_points = [each for each in minimas[0]]\n",
    "text = ''\n",
    "for num, each in enumerate(df['sentence']):\n",
    "    if num in split_points:\n",
    "        text += f'\\n\\n{each} '\n",
    "    else:\n",
    "        text += f'{each} '\n",
    "        \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03f787",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "Trying to summarize the text to see if something could be built upon the summarization. I tried several approaches (like LSA), but settled on TextRank. Also, while searching, found a funny and compact library called sumy.\n",
    "\n",
    "### Extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "def summarize_with_textrank(text, sentences_count=10):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "\n",
    "    text_rank_summarizer = TextRankSummarizer()\n",
    "    text_rank_summary = text_rank_summarizer(parser.document, sentences_count=sentences_count)\n",
    "    summary_text = \"\\n\".join(str(sentence) for sentence in text_rank_summary)\n",
    "    \n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc321fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary = summarize_with_textrank(text, 3)  # top 3\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4941f",
   "metadata": {},
   "source": [
    "### Clustering and extracting sub-themes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a710a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.utils import get_stop_words\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "stop_words = set(get_stop_words('ENGLISH'))  # very nice stop words collection\n",
    "\n",
    "\n",
    "def get_text_theme_keywords(sentences, embeddings, num_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Getting closest sentences\n",
    "    cluster_sentences = [[] for _ in range(num_clusters)]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        cluster_sentences[cluster_labels[i]].append(sentence)\n",
    "    \n",
    "    # Getting most used tokens, excluding stop-words\n",
    "    cluster_keywords = []\n",
    "    for cluster in cluster_sentences:\n",
    "        cluster_text = ' '.join(cluster)\n",
    "        cluster_words = clean_tokenize(cluster_text)\n",
    "        cluster_words = [word for word in cluster_words if word not in stop_words]\n",
    "        word_counts = Counter(cluster_words)\n",
    "        most_common_words = word_counts.most_common(3)\n",
    "        cluster_keywords.append([word[0] for word in most_common_words])\n",
    "    \n",
    "    return cluster_keywords\n",
    "\n",
    "text_theme_keywords = get_text_theme_keywords(df['sentence'].tolist(), df['embedding'].tolist())\n",
    "print(\"Text keywords:\", text_theme_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed3db5",
   "metadata": {},
   "source": [
    "Couldn't derive subthemes, but worked **amazing** for the whole text (num_clusters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcb7d0",
   "metadata": {},
   "source": [
    "## Enrichment from YouTube \n",
    "Let's see if I can extract something meaningful from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdab662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "YOUTUBE_API_KEY = 'YOUR_KEY'\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "video_id = 'IMfBS4mBfBQ'\n",
    "\n",
    "\n",
    "def get_comments(video_id):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100,\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = {\n",
    "            \"id\": item['snippet']['topLevelComment']['id'],\n",
    "            \"text\": item['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "            \"likes\": item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "        }\n",
    "        comments.append(comment)\n",
    "        \n",
    "    return sorted(comments, key=lambda item: item[\"likes\"], reverse=True)\n",
    "\n",
    "\n",
    "def get_channel_id(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' in response and response['items']:\n",
    "        channel_id = response['items'][0]['snippet']['channelId']\n",
    "        return channel_id\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_channel_description(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='snippet',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' in response and response['items']:\n",
    "        description = response['items'][0]['snippet']['description']\n",
    "        return description\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_channel_videos_descriptions(channel_id):\n",
    "    video_descriptions = []\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        channelId=channel_id,\n",
    "        maxResults=50,  \n",
    "        order=\"date\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        if item['id']['kind'] == \"youtube#video\":\n",
    "            video_description = {\n",
    "                \"title\": item['snippet']['title'],\n",
    "                \"description\": item['snippet']['description']\n",
    "            }\n",
    "            video_descriptions.append(video_description)\n",
    "\n",
    "    return video_descriptions\n",
    "\n",
    "channel_id = get_channel_id(video_id)\n",
    "comments = get_comments(video_id)\n",
    "description = get_channel_description(channel_id)\n",
    "video_descriptions = get_channel_videos_descriptions(channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a760cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.DataFrame(comments)\n",
    "comments_df.columns = ['id', 'comment', 'likes']\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1ba2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "videos_df = pd.DataFrame(video_descriptions)\n",
    "videos_df.columns = ['title', 'description']\n",
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "path = \"./\"\n",
    "\n",
    "def download_video(video_id, save_path=path):\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    yt = YouTube(video_url)\n",
    "    stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "    if stream:\n",
    "        stream.download(output_path=save_path, filename=video_id + '.mp4')\n",
    "        print(f'Video {video_id} has been downloaded successfully.')\n",
    "    else:\n",
    "        print('No suitable stream found for downloading.')\n",
    "\n",
    "\n",
    "download_video(video_id=video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cde5be",
   "metadata": {},
   "source": [
    "Here, I was already thinking about the structure of the video I aim to create. I searched the channel to see if there are any videos containing the _author's_ intro. It would make sense to include them into a final resut. I found such videos, but they were intros of other people, not the blogger himself, and I couldn't automatically distinguish them, so I had to give up the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_channel_intro(channel_id):\n",
    "\n",
    "    # Videos from channels \n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        channelId=channel_id,\n",
    "        maxResults=50,\n",
    "        order=\"date\",\n",
    "        type=\"video\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    # Looking for videos with intros\n",
    "    for item in response.get('items', []):\n",
    "        title = item['snippet']['title'].lower()\n",
    "        description = item['snippet']['description'].lower()\n",
    "        \n",
    "        # Keywords for search \n",
    "        keywords = ['intro', 'introduction', 'about', 'welcome', 'hello', 'hi']\n",
    "        \n",
    "        if any(keyword in title for keyword in keywords) or any(keyword in description for keyword in keywords):\n",
    "            video_id = item['id']['videoId']\n",
    "            video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "            print(f'Found potential intro video: {video_url}')\n",
    "            return video_url\n",
    "    \n",
    "    print('Intro video not found.')\n",
    "    return None\n",
    "\n",
    "\n",
    "find_channel_intro(channel_id=channel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbb628",
   "metadata": {},
   "source": [
    "I once took a course on how to make reels on Instagram. The video structure in that course was actually based on text (I never thought that experience would come in handy). The suggested structure was as follows:\n",
    "\n",
    "- Title\n",
    "- Hook\n",
    "- Intro\n",
    "- Core\n",
    "- Conclusion\n",
    "\n",
    "An example of such a video (you've definitely come across them online):\n",
    "- Title: All the best business books are actually about the same thing\n",
    "- Hook: Yes, you're about to hear one key idea that is mentioned in all the top business literature\n",
    "- Intro: I'm Alex, this is a blog about money on Instagram, subscribe!\n",
    "- Core: So, the idea. Business is not the company's logo, office, or business cards. And it's not even a team of employees, a strong product, or followers on social media. Business is when you get paid. THAT'S IT.\n",
    "- Conclusion: If there's an incoming flow of money, you have a business. If not, then not yet.\n",
    "\n",
    "I decided to try to assemble something similar.\n",
    "\n",
    "Approaches:\n",
    "1. Highlights-based: I can take some emotional moment from the video and build my extract around it.\n",
    "2. Title-based: I can look for what the video is about and rely on a sentence with the video's theme.\n",
    "3. Question-based: I can look for question-answer pairs.\n",
    "4. Intro-based: I can look for intros on the channel. As I found out, these can be not only the author's intros, but why not.\n",
    "4. I can get the most commented parts of the video (possible, but not implemented)\n",
    "\n",
    "Let's try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a1322",
   "metadata": {},
   "source": [
    "## Experimenting with GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gpt(model=\"text-davinci-003\", temperature=0.7, max_tokens=150):\n",
    "    \n",
    "    prompt = \"\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt_text,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    generated_text = response.choices[0].text.strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47111ab5",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Decided to validate the output on the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82c36b",
   "metadata": {},
   "source": [
    "## Cutting and editing final video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52badd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "\n",
    "def cut_sentences_from_video(path, df, sentence_numbers):\n",
    "    video = VideoFileClip(path)\n",
    "    clips = []\n",
    "\n",
    "    for number in sentence_numbers:\n",
    "        start_time = df.loc[df.index == number, 'start_time'].values[0]\n",
    "        end_time = df.loc[df.index == number, 'end_time'].values[0]\n",
    "        clip = video.subclip(start_time, end_time)\n",
    "        clips.append(clip)\n",
    "\n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    final_clip_path = 'output_video.mp4'\n",
    "    final_clip.write_videofile(final_clip_path, codec=\"libx264\", fps=24)\n",
    "\n",
    "\n",
    "cut_sentences_from_video('/Users/mariachakchurina/projects/video_transcript_analysis/IMfBS4mBfBQ.mp4', df, [49, 50, 51, 54, 55])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a0675",
   "metadata": {},
   "source": [
    "### Seeing final text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [6, 11, 36, 37, 53, 63, 64, 80]\n",
    "generated_text = ' '.join(raw_df.loc[selected, 'sentence'])\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
