{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa361f0e",
   "metadata": {},
   "source": [
    "# Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122446fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariachakchurina/projects/video_transcript_analysis/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fe4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       In some ways, the point of LinkedIn is obvious.\n",
      "1     It's not like Instagram, where you're supposed...\n",
      "2     It's not like Twitter, where you're supposed t...\n",
      "3     And it's not like Facebook, where you're suppo...\n",
      "4        LinkedIn, however, is where you go to network.\n",
      "                            ...                        \n",
      "81    You know, Dan, I have to say I've been so incr...\n",
      "82                   I think you do have what it takes.\n",
      "83                                    Julie, thank you.\n",
      "84                           Thank you for saying that.\n",
      "85    I'll pay you the $15 I promised you for saying...\n",
      "Name: sentence, Length: 86, dtype: object\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"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\"\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "transcript_files = [\n",
    "    \"2024 Rolls-Royce Spectre Review.csv\",\n",
    "    \"Apple Vision Pro Impressions.csv\",\n",
    "    \"George Hotz.csv\",\n",
    "    \"The END of Sam Bankman Fried.csv\",\n",
    "    \"Why is LinkedIn so weird.csv\"\n",
    "]\n",
    "\n",
    "folder_path = \"data/transcripts\"\n",
    "\n",
    "file_path = os.path.join(folder_path, transcript_files[4])\n",
    "raw_df = pd.read_csv(file_path)\n",
    "raw_df.rename(columns={'length': 'time'}, inplace=True)\n",
    "\n",
    "print(raw_df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text):\n",
    "    \"\"\"Removes punctuation, converts to lowercase, and splits into words.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "raw_df['tokens'] = raw_df['sentence'].apply(clean_tokenize)\n",
    "raw_df['length'] = raw_df['tokens'].apply(len)\n",
    "print(raw_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=API_KEY,  # todo https://github.com/openai/openai-python/discussions/742 os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=MODEL\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "raw_df['embedding'] = raw_df['sentence'].apply(get_embeddings)\n",
    "print(raw_df['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(embeddings):\n",
    "    cos_distances = [None]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        cos_distance = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]  # todo np dot \n",
    "        cos_distances.append(cos_distance)\n",
    "    return cos_distances\n",
    "\n",
    "raw_df['cos_dist'] = cosine_distance(raw_df['embedding'].tolist())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(raw_df['cos_dist'], marker='o', linestyle='-')\n",
    "plt.xlabel('Sentence Index')\n",
    "plt.ylabel('Cosine Distance')\n",
    "plt.title('Cosine Distance Between Consecutive Sentences')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = 35\n",
    "short = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = raw_df['cos_dist'].quantile(0.8)\n",
    "close_indices = raw_df.index[raw_df['cos_dist'] > quantile].tolist()\n",
    "# print(close_indices)\n",
    "\n",
    "sentences = [raw_df.loc[0, 'sentence']]\n",
    "times = [raw_df.loc[0, 'time']]\n",
    "\n",
    "i = 1\n",
    "while i < len(raw_df):    \n",
    "    current = raw_df.loc[i, 'sentence']\n",
    "    current_t = raw_df.loc[i, 'time']\n",
    "    length = raw_df.loc[i, 'length']\n",
    "\n",
    "    previous = sentences[-1]\n",
    "    previous_t = times[-1]\n",
    "    \n",
    "    # if it's short and similar, concatenate it \n",
    "    if i in close_indices and length <= short:\n",
    "        sentences[-1] = previous + \" \" + current\n",
    "        times[-1] = previous_t + current_t\n",
    "        \n",
    "    # if starts with ..., concatenate it\n",
    "    elif previous.endswith('...') and current.startswith('...'):\n",
    "        sentences[-1] = previous[:-3] + \" \" + current[3:]\n",
    "        times[-1] = previous_t + current_t\n",
    "    \n",
    "    # leave it as is if it's fine \n",
    "    else:        \n",
    "        sentences.append(current)\n",
    "        times.append(current_t)\n",
    "    i += 1\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df['sentence'] = sentences\n",
    "df['time'] = times\n",
    "\n",
    "print(df[-10:-1])\n",
    "print(\"\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['sentence'].apply(clean_tokenize)\n",
    "df['tempo'] = df['tokens'].apply(len) / df['time']\n",
    "df['length'] = df['tokens'].apply(len)\n",
    "df['question'] = df['sentence'].str.contains('\\?')\n",
    "df['embedding'] = df['sentence'].apply(get_embeddings)\n",
    "\n",
    "df.info()\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_df['sentence'][49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем колонки с временем начала и окончания предложений\n",
    "\n",
    "start_times = [0]\n",
    "end_times = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i > 0:\n",
    "        start_time = start_times[i-1] + df.loc[i-1, 'time']\n",
    "        start_times.append(start_time)\n",
    "    end_time = start_times[i] + df.loc[i, 'time']\n",
    "    end_times.append(end_time)\n",
    "\n",
    "df['start_time'] = start_times\n",
    "df['end_time'] = end_times\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[74])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1da5c3",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        scores = softmax(logits, dim=1)\n",
    "        scores_dict = {label: score.item() for label, score in zip(['negative', 'neutral', 'positive'], scores[0])}\n",
    "        return scores_dict\n",
    "    \n",
    "    def apply_to_dataframe(self, df, text_column):\n",
    "        non_neutrals, positives, negatives = [], [], []\n",
    "        \n",
    "        for text in df[text_column]:\n",
    "            sentiment_scores = self.predict_sentiment(text)\n",
    "            non_neutrals.append(1 - sentiment_scores['neutral'])\n",
    "            positives.append(sentiment_scores['positive'])\n",
    "            negatives.append(sentiment_scores['negative'])\n",
    "            \n",
    "        df['emotion_score'] = non_neutrals\n",
    "        df['positive_score'] = positives\n",
    "        df['negative_score'] = negatives\n",
    "\n",
    "\n",
    "analyzer = SentimentAnalyzer()\n",
    "analyzer.apply_to_dataframe(df, 'sentence')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e34620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9af97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tempo = df['tempo'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e598509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_and_tempo(df):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.plot(df.index, df['emotion_score'], color='red', label='Roberta')\n",
    "    \n",
    "    plt.plot(df.index, df['tempo'], color='green', label='Tempo')\n",
    "    plt.axhline(y=mean_tempo, color='green', linestyle='-', label='Mean tempo')\n",
    "\n",
    "    plt.xlabel('Sentence Number')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Polarity, Subjectivity, and Tempo across Sentences')\n",
    "    plt.legend()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['question']:\n",
    "            plt.axvline(x=index, color='blue', label='Question Mark', linestyle='--')\n",
    "    \n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    plt.xticks(df.index[::2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_sentiment_and_tempo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_non_neutral_indices = sorted(df['emotion_score'].abs().nlargest(8).index.tolist())\n",
    "print(\"Top 8 RoBERTa Non-Neutral Sentences:\", top_non_neutral_indices)\n",
    "\n",
    "print(\"\\n\")\n",
    "for index in top_non_neutral_indices:\n",
    "    print(f\"{index}: {df.loc[index, 'sentence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_threshold = df['emotion_score'].quantile(0.5)  # Подумай над цифрой \n",
    "\n",
    "questions = df[(df['question']) & (df['emotion_score'] > emotional_threshold)]  # todo quartile подумай над метрикой — эта или другая? \n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    print(f\"{index}: {row['sentence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastest = df['tempo'].quantile(0.75)\n",
    "\n",
    "fastest_emotional = df[(df['emotion_score'] > emotional_threshold) & (df['tempo'] > fastest)]\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    print(f\"{index}: {row['sentence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e411c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_5_slowest = df.sort_values(by='tempo', ascending=True).head(5).index.tolist()  # todo некрасиво \n",
    "\n",
    "# todo: внимательно с индексами, свалится \n",
    "# todo: пауза неплохо подчеркивает эмоции, можно выбрать что-то отсюда \n",
    "for i in top_5_slowest:\n",
    "    print(f\"{i - 2}: {df.loc[i - 2, 'sentence']}\")\n",
    "    print(f\"{i - 1}: {df.loc[i - 1, 'sentence']}\")\n",
    "    print(f\"{i}: {df.loc[i, 'sentence']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfee7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_closest_statements(df):\n",
    "    questions_df = df[df['question'] == True]\n",
    "    statements_df = df[df['question'] == False]\n",
    "    \n",
    "    question_embeddings = list(questions_df['embedding'])\n",
    "    statement_embeddings = list(statements_df['embedding'])\n",
    "    \n",
    "    closest_statements = {}\n",
    "    \n",
    "    for index, question_embedding in questions_df.iterrows():\n",
    "        similarities = cosine_similarity([question_embedding['embedding']], statement_embeddings)\n",
    "        top_5_indices = similarities[0].argsort()[-3:][::-1]  # Получаем индексы самых похожих\n",
    "        \n",
    "#         top_5_indices = np.append(np.sort(top_5_indices[1:]), top_5_indices[0])\n",
    "        \n",
    "        closest_sentences = statements_df.iloc[top_5_indices]['sentence'].values\n",
    "        closest_statements[question_embedding['sentence']] = closest_sentences\n",
    "    \n",
    "    return closest_statements\n",
    "\n",
    "closest_statements = find_closest_statements(df)\n",
    "for question, statements in closest_statements.items():\n",
    "    text = ' '.join(statements)\n",
    "    print(f\"Question: {question}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9012a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_threshold=0.5\n",
    "emotional_indexes = df[df['emotion_score'] > emotion_threshold].index\n",
    "emotional_indexes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intros(df):\n",
    "    similarity_threshold=0.765  # todo ad-hoc threshold, very sorry\n",
    "    \n",
    "    request = \"My name is Ankit Singla and I'm a full-time blogger. I blog about blogging. I'm Karen, an entrepreneur and VC consultant. Paul Erdős was a Hungarian mathematician. He was one of the most prolific mathematicians and producers of mathematical conjectures of the 20th century. This is Maria and she is a Data Engineer at Rask\"\n",
    "    request_embedding = get_embeddings(request)\n",
    "    request_embedding = np.array(request_embedding).reshape(1, -1)  # Подготавливаем вектор запроса\n",
    "\n",
    "    sentence_similarities = []\n",
    "    for index, row in df.iterrows():\n",
    "        embedding = np.array(row['embedding']).reshape(1, -1)  # Подготавливаем вектор предложения\n",
    "        similarity = cosine_similarity(embedding, request_embedding)[0][0]\n",
    "#         sentence_similarities.append((index, row['sentence'], similarity))\n",
    "        if similarity > similarity_threshold:\n",
    "            sentence_similarities.append((index, row['sentence'], similarity))\n",
    "\n",
    "    sorted_sentences = sorted(sentence_similarities, key=lambda x: x[2], reverse=True)\n",
    "    print(sorted_sentences)\n",
    "    return [{i: sentence} for i, sentence, _ in sorted_sentences]\n",
    "\n",
    "intros = find_intros(df)\n",
    "print(intros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7f900",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "todo: Кластеризация не дает результатов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caacc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "embeddings_array = np.array(list(df['embedding']))\n",
    "\n",
    "# Getting the optimal number of clusters using silhouette score\n",
    "silhouette_scores = []\n",
    "for n_clusters in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings_array)\n",
    "    score = silhouette_score(embeddings_array, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_clusters = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "for cluster in sorted(df['cluster'].unique()):\n",
    "    sentences = df[df['cluster'] == cluster].sort_index()['sentence']\n",
    "    for sentence in sentences:\n",
    "        print(f\"{sentence}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39053e99",
   "metadata": {},
   "source": [
    "# Разбиение на абзацы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042e480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_matrix = np.array(df['embedding'].tolist())\n",
    "cosine_sim_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# cosine_sim_matrix теперь содержит косинусное сходство между всеми парами эмбеддингов\n",
    "sns.heatmap(cosine_sim_matrix).set_title('Cosine similarities matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661450c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Поиск точек разбиения \n",
    "\n",
    "def rev_sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + math.exp(0.5 * x))\n",
    "\n",
    "def activate_similarities(similarities: np.array, p_size=10) -> np.array:\n",
    "    x = np.linspace(-10, 10, p_size)\n",
    "    y = np.vectorize(rev_sigmoid)\n",
    "    activation_weights = np.pad(y(x), (0, similarities.shape[0] - p_size), 'constant')\n",
    "    diagonals = [similarities.diagonal(each) for each in range(1, similarities.shape[0])]\n",
    "    diagonals = [np.pad(each, (0, similarities.shape[0] - len(each)), 'constant') for each in diagonals]\n",
    "    diagonals = np.stack(diagonals)\n",
    "    diagonals = diagonals * activation_weights[:diagonals.shape[0]].reshape(-1, 1)\n",
    "    activated_similarities = np.sum(diagonals, axis=0)\n",
    "    return activated_similarities\n",
    "\n",
    "activated_similarities = activate_similarities(cosine_sim_matrix, p_size=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "minimas = argrelextrema(activated_similarities, np.less, order=2)\n",
    "sns.lineplot(y=activated_similarities, x=range(len(activated_similarities)), ax=ax).set_title('Relative minima')\n",
    "plt.vlines(x=minimas, ymin=min(activated_similarities), ymax=max(activated_similarities), colors='purple', ls='--', lw=2, label='Split Points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_points = [each for each in minimas[0]]\n",
    "text = ''\n",
    "for num, each in enumerate(df['sentence']):\n",
    "    if num in split_points:\n",
    "        text += f'\\n\\n{each} '\n",
    "    else:\n",
    "        text += f'{each} '\n",
    "        \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03f787",
   "metadata": {},
   "source": [
    "# Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "def summarize_with_textrank(text, sentences_count=10):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "\n",
    "    # Used TextRank for summarization\n",
    "    text_rank_summarizer = TextRankSummarizer()\n",
    "    text_rank_summary = text_rank_summarizer(parser.document, sentences_count=sentences_count)\n",
    "    summary_text = \"\\n\".join(str(sentence) for sentence in text_rank_summary)\n",
    "    \n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc321fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary = summarize_with_textrank(text, 3)  # todo: also tried LSA \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a710a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.utils import get_stop_words\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "stop_words = set(get_stop_words('ENGLISH'))  # todo remove \n",
    "\n",
    "# todo refactor \n",
    "def get_text_theme_keywords(sentences, embeddings, num_clusters=1):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Собираем предложения для каждого кластера\n",
    "    cluster_sentences = [[] for _ in range(num_clusters)]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        cluster_sentences[cluster_labels[i]].append(sentence)\n",
    "    \n",
    "    # Для каждого кластера выбираем наиболее часто встречающиеся слова, исключая стоп-слова\n",
    "    cluster_keywords = []\n",
    "    for cluster in cluster_sentences:\n",
    "        cluster_text = ' '.join(cluster)\n",
    "        cluster_words = clean_tokenize(cluster_text)\n",
    "        cluster_words = [word for word in cluster_words if word not in stop_words]\n",
    "        word_counts = Counter(cluster_words)\n",
    "        most_common_words = word_counts.most_common(3)\n",
    "        cluster_keywords.append([word[0] for word in most_common_words])\n",
    "    \n",
    "    return cluster_keywords\n",
    "\n",
    "text_theme_keywords = get_text_theme_keywords(df['sentence'].tolist(), df['embedding'].tolist())\n",
    "print(\"Ключевые слова темы текста:\", text_theme_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75d8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60fcb7d0",
   "metadata": {},
   "source": [
    "# Обогащение с YouTube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdab662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# todo: reconfig\n",
    "\n",
    "YOUTUBE_API_KEY = 'AIzaSyAAD5vikUMvBwj1xUyDW4YyGKneIQvdk_U'\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "video_id = 'IMfBS4mBfBQ'\n",
    "\n",
    "\n",
    "def get_comments(video_id):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100,\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = {\n",
    "            \"id\": item['snippet']['topLevelComment']['id'],\n",
    "            \"text\": item['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "            \"likes\": item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "        }\n",
    "        comments.append(comment)\n",
    "        \n",
    "    return sorted(comments, key=lambda item: item[\"likes\"], reverse=True)\n",
    "\n",
    "\n",
    "def get_channel_id(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' in response and response['items']:\n",
    "        channel_id = response['items'][0]['snippet']['channelId']\n",
    "        return channel_id\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_channel_description(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='snippet',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' in response and response['items']:\n",
    "        description = response['items'][0]['snippet']['description']\n",
    "        return description\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_channel_videos_descriptions(channel_id):\n",
    "    video_descriptions = []\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        channelId=channel_id,\n",
    "        maxResults=50,  \n",
    "        order=\"date\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        if item['id']['kind'] == \"youtube#video\":\n",
    "            video_description = {\n",
    "                \"title\": item['snippet']['title'],\n",
    "                \"description\": item['snippet']['description']\n",
    "            }\n",
    "            video_descriptions.append(video_description)\n",
    "\n",
    "    return video_descriptions\n",
    "\n",
    "channel_id = get_channel_id(video_id)\n",
    "comments = get_comments(video_id)\n",
    "description = get_channel_description(channel_id)\n",
    "video_descriptions = get_channel_videos_descriptions(channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a760cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.DataFrame(comments)\n",
    "comments_df.columns = ['id', 'comment', 'likes']\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1ba2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "videos_df = pd.DataFrame(video_descriptions)\n",
    "videos_df.columns = ['title', 'description']\n",
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "path = \"./\"  # todo \n",
    "\n",
    "def download_video(video_id, save_path=path):\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    yt = YouTube(video_url)\n",
    "    stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "    if stream:\n",
    "        stream.download(output_path=save_path, filename=video_id + '.mp4')\n",
    "        print(f'Video {video_id} has been downloaded successfully.')\n",
    "    else:\n",
    "        print('No suitable stream found for downloading.')\n",
    "\n",
    "\n",
    "download_video(video_id=video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_channel_intro(channel_id):\n",
    "\n",
    "    # Получаю список видео канала\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        channelId=channel_id,\n",
    "        maxResults=50,\n",
    "        order=\"date\",\n",
    "        type=\"video\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    # Поиск видео, в которых может быть интро \n",
    "    for item in response.get('items', []):\n",
    "        title = item['snippet']['title'].lower()\n",
    "        description = item['snippet']['description'].lower()\n",
    "        \n",
    "        # Ключевые слова для поиска в заголовках и описаниях\n",
    "        keywords = ['intro', 'introduction', 'about', 'welcome', 'начало', 'приветствие']\n",
    "        \n",
    "        # Проверяем наличие ключевых слов в заголовках и описаниях\n",
    "        if any(keyword in title for keyword in keywords) or any(keyword in description for keyword in keywords):\n",
    "            video_id = item['id']['videoId']\n",
    "            video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "            print(f'Found potential intro video: {video_url}')\n",
    "            return video_url\n",
    "    \n",
    "    print('Intro video not found.')\n",
    "    return None\n",
    "\n",
    "\n",
    "find_channel_intro(channel_id=channel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbb628",
   "metadata": {},
   "source": [
    "# Forming sequence \n",
    "\n",
    "Структура\n",
    "- Заголовок\n",
    "- Хук \n",
    "- Интро\n",
    "- Кода \n",
    "- Вывод\n",
    "\n",
    "Что это значит \n",
    "- Заголовок: все лучшие книги по бизнесу на самом деле про одно и то же \n",
    "- Хук: да, сейчас ты услышишь одну ключевую мысль, о которой говорится во всей топовой деловой литературе -- строчка, задача которой -- удержать внимание \n",
    "- Интро: Я Артем, здесь про деньги в инстаграм, подписывайся \n",
    "- Кода: итак, мысль. Бизнес -- это не логотип компании, офис или визитные карточки. И это даже не команда сотрудников, сильный продукт или подписчики в соцсетях. Бизнес -- это когда тебе платят. ВСЁ\n",
    "- Вывод: если есть входящий поток денег -- у тебя бизнес, если нет, то пока еще нет\n",
    "\n",
    "Подходы \n",
    "1. Хайлайтс-бейсд \n",
    "2. Заголовок-бейсд\n",
    "3. Вопрос-бейсд \n",
    "4. Интро-бейсд "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gpt(model=\"text-davinci-003\", temperature=0.7, max_tokens=150):\n",
    "    \n",
    "    prompt = \"\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt_text,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    generated_text = response.choices[0].text.strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a031a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44225d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2755dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47111ab5",
   "metadata": {},
   "source": [
    "# Валидация \n",
    "\n",
    "todo: на порождающей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa638c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d82c36b",
   "metadata": {},
   "source": [
    "# Нарезка и склейка "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "\n",
    "def cut_sentences_from_video(path, df, sentence_numbers):\n",
    "    video = VideoFileClip(path)\n",
    "    clips = []\n",
    "\n",
    "    for number in sentence_numbers:\n",
    "        start_time = df.loc[df.index == number, 'start_time'].values[0]\n",
    "        end_time = df.loc[df.index == number, 'end_time'].values[0]\n",
    "        clip = video.subclip(start_time, end_time)\n",
    "        clips.append(clip)\n",
    "\n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    final_clip_path = 'output_video.mp4'\n",
    "    final_clip.write_videofile(final_clip_path, codec=\"libx264\", fps=24)\n",
    "\n",
    "\n",
    "cut_sentences_from_video('/Users/mariachakchurina/projects/video_transcript_analysis/IMfBS4mBfBQ.mp4', df, [49, 50, 51, 54, 55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c1714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So why the hell is it so weird? At what age should your child start dropshipping? So why in the hell would anyone want to go viral in this sea of cringe? You know, it's so important today for C-level executives to have a presence on LinkedIn, right? And what kind of quality content does a LinkedIn non-influencer put out? So if this is the state of LinkedIn now, where is it headed? I mean, I think it's going to continue to grow in importance as, you know, we become a global economy, right? From this conversation, do you think I have what it takes to be a thought leader?\n"
     ]
    }
   ],
   "source": [
    "selected = [6, 11, 36, 37, 53, 63, 64, 80]\n",
    "generated_text = ' '.join(raw_df.loc[selected, 'sentence'])\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
